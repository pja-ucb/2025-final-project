{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJmgxZ_-LcoX"
   },
   "source": [
    "## Fine-tuning a Seq2Seq model (FLAN-T5) in Colab with limited RAM\n",
    "\n",
    "This notebook shows how to fine-tune a sequence-to-sequence (encoder-decoder) model, using FLAN-T5, on a new task. It also focuses on how to avoid running out of memory by loading part of your data at a time while you train (using a Huggingface streaming dataset), and saving model checkpoints as you go.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-fall-main/blob/master/materials/walkthrough_notebooks/fine_tuning_with_limited_ram/fine_tune_t5_with_limited_ram.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMWJ_MH1Lgsg",
    "outputId": "867fe783-6cad-4d88-847c-14ba707a6cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U datasets\n",
    "!pip install -q sentencepiece\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4fKpWjWlMJZo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VvqKWukMdtV"
   },
   "source": [
    "### Data\n",
    "\n",
    "To fine-tune T5, we'll use a small dataset for translating Shakespeare to modern English. You can [download the dataset here](https://github.com/cocoxu/Shakespeare), and save it to someplace in your Drive. In the next cells, we'll mount our Drive folder and load the data from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hId0nV1UMJct",
    "outputId": "151041c3-8d1a-4444-d20e-bb9ac6aa9e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# This cell will authenticate you and mount your Drive in the Colab.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rG6QmmqWLn6a"
   },
   "outputs": [],
   "source": [
    "# Modify this path to where you saved the Shakespear data in your Drive.\n",
    "text_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_plays-org-mod.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02mLGzHzGgIB",
    "outputId": "bd4d0717-c65f-490b-e0c0-6b9f0932f69e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 123M\n",
      "-rw------- 1 root root 651K Mar 13 23:11 embold_github_dev.csv\n",
      "-rw------- 1 root root  24M Mar 13 23:11 embold_github_train.csv\n",
      "-rw------- 1 root root  94M Mar 13 23:09 embold_train.json\n",
      "-rw------- 1 root root 261K Mar 15 05:46 HerdOfModelsPaperTable.png\n",
      "-rw------- 1 root root 440K Jun 29 18:56 test_pairs.csv\n",
      "-rw------- 1 root root 2.1M Jun 29 18:56 train_pairs.csv\n",
      "-rw------- 1 root root 2.1M Aug 13  2023 train_plays-org-mod.txt\n",
      "-rw------- 1 root root 459K Jun 29 18:56 valid_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "#confirm we can access the file\n",
    "!ls -lh drive/MyDrive/ISchool/MIDS/266/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t_i8h2C2Mcyd"
   },
   "outputs": [],
   "source": [
    "# Read the data, which is in one txt file, with one example per line.\n",
    "# Each example is a pair of Shakespearean and modern English sentences, separated by a tab.\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "\n",
    "prefix = 'Translate the following text to modern English: '\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    orig, target = line.split('\\t')\n",
    "    text_pairs.append({'orig': prefix + orig, 'target': target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDHlRNR0Mc1D",
    "outputId": "243e24fb-e740-45d8-ef5f-0df783c74b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig': 'Translate the following text to modern English: Away!', 'target': 'Let’s go!'}\n",
      "{'orig': \"Translate the following text to modern English: Get your apparel together; good strings to your beards, new ribbons to your pumps; meet presently at the palace; every man look o'er his part; for the short and the long is, our play is preferred.\", 'target': 'Get your costumes together; good strings to your beards, new ribbons to your shoes; meet presently at the palace; every man look over his part; for the short and the long of it is, they want to see our play.'}\n",
      "{'orig': 'Translate the following text to modern English: Conrade, I say!', 'target': 'Conrade, I say!'}\n",
      "{'orig': 'Translate the following text to modern English: Kent banished thus?', 'target': 'Kent’s been banished just like that?'}\n",
      "{'orig': 'Translate the following text to modern English: He hath fought today As if a god, in hate of mankind, had Destroyed in such a shape.', 'target': 'Today he fought as though he were a god who hated mankind.'}\n"
     ]
    }
   ],
   "source": [
    "# Look at some examples\n",
    "for _ in range(5):\n",
    "    print(np.random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGbY61KYMlWo",
    "outputId": "ce6fd837-100b-4319-a762-cf74024dfc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19088 total pairs\n",
      "13362 training pairs\n",
      "2863 validation pairs\n",
      "2863 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Let's create some splits\n",
    "np.random.shuffle(text_pairs)\n",
    "num_valid_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_valid_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "valid_pairs = text_pairs[num_train_samples : num_train_samples + num_valid_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_valid_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(valid_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9BQMq-BiPQaG"
   },
   "outputs": [],
   "source": [
    "# Save this because we'll need to tell the trainer how many examples we have\n",
    "num_train_examples = len(train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B3UO4xczMlZe"
   },
   "outputs": [],
   "source": [
    "# Save splits to separate csv files, to load only part at a time later\n",
    "train_filepath = 'drive/MyDrive/ISchool/MIDS/266/data/train_pairs.csv'\n",
    "valid_filepath = 'drive/MyDrive/ISchool/MIDS/266/data/valid_pairs.csv'\n",
    "test_filepath = 'drive/MyDrive/ISchool/MIDS/266/data/test_pairs.csv'\n",
    "\n",
    "pd.DataFrame(train_pairs).to_csv(train_filepath, index=False)\n",
    "pd.DataFrame(valid_pairs).to_csv(valid_filepath, index=False)\n",
    "pd.DataFrame(test_pairs).to_csv(test_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c-D9kftzSIG9"
   },
   "outputs": [],
   "source": [
    "# Remove the full dataset from memory\n",
    "train_pairs = None\n",
    "valid_pairs = None\n",
    "test_pairs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSMjkc5uODm7"
   },
   "source": [
    "The code below loads the pretrained T5 pytorch model and tokenizer. We'll need the tokenizer before we write our preprocessing function, which we'll map to the streaming datasets, so let's load them now.\n",
    "\n",
    "We'll use FLAN-T5, which is a version of T5 that has been fine-tuned on a collection of instruction tuning datasets. This makes it better than the original T5 at learning new tasks, when provided with a natural language instruction prompt explaining the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279,
     "referenced_widgets": [
      "b184470bf02a4fecbc5e9f1f6eccb7e9",
      "53e2eff817404702933ca031d81211d7",
      "89b404446ec74e3ea9e8c9bb8696c7f5",
      "acf1d7073a734988b616ffb3d00a6f45",
      "99c99619d93b4b298925e7a5005a2262",
      "9082ce48a2504591bd6245eda0d0ebea",
      "bfd5e7ecb91947f285d00655b04acaf6",
      "1de4cdff0e774025a432e99f3de57e7c",
      "22ac2cbd513d4765a598b17b33b7d54e",
      "21772763d93a48d189606b200210ae71",
      "217a9f60ba3d4e559f2467315ab50940",
      "811f153d8d86426c851a7e9263a0cbb0",
      "222f4f7540c749639f029fd98ca0b9e5",
      "a249804869c142f4ac9ef543c11754e5",
      "63dbd1240eaa4241a0c3f67b36047b7e",
      "ec4375577a6141fd9d71511dc1f32ecb",
      "d05186782d3c4d89ad1837ebaf31fbc8",
      "60a744c1fecc4ff5a3e37ea4791f01f8",
      "88f8db459bbf4627978ae2331bfba882",
      "a5f014da882747d5ada623e4793d34e2",
      "47fb95e9cb384c279aee04c250c76f25",
      "619d0453a1a24f52b5367655cccf6ba4",
      "dbb73adb3c7e4f758bc706df25e655ad",
      "ec96c6eff7e94441b95bd6946db28b5c",
      "a65a1334973e4aa7b04cc75e8fb55450",
      "a5e15db6c84c4c40ab05123513180c7c",
      "01d3aba576aa4f1d9fe9e3ee384bc529",
      "817d6abb2c1349a3a65261291cb2f3be",
      "78c85f4e8f1d42dfb96581353f38dc2b",
      "db259a12407448cf91a3275e431ccc22",
      "8366679edcc84ffdabdf4f8f4192bb4c",
      "f2451c0a202948c78273f4944e75b647",
      "2a699a5245014055a1a2b10e3277776b",
      "916abdcfafc94e9d80d9f853ada87866",
      "18571126b49a4f37b88b079c92433621",
      "b1ed2b1ae3a64e5f848f8778c4f7943d",
      "f0dbf11e13d34b49973a5d56f9676b38",
      "6a0accce92e545039abcc9351685fb14",
      "53b92a2b63f542cd9e52d583cbb71a2f",
      "556752fa726442db8b94105938eca2bc",
      "1d3e0445edc043c5a866219e96df87e5",
      "0f2c845187104a92b18f4ebddc51ce83",
      "bf16027662554299957e56c6fa0328fd",
      "c35ecfa1f98c429fb1dfe2330b48ea94",
      "19998a797a2d46acbe5f4c2a7297bf8f",
      "081c08ae66dc47a69573175544a01cd7",
      "7db923af2c8d455d89f772297cd3e326",
      "53af1865e5e34858ab6b068821e76ab7",
      "13f6fb3deff04b3aaf05237f441b39f3",
      "35e25d5f06244c89a6b9130f205c79b5",
      "4e97727a1ecc4a7f8f0f95ce250877a6",
      "90631e845f724517a627878bae6c6375",
      "e1b4d36f45ab4879b391e3b540ead3ec",
      "cbd63bd511f64a5883bc3e68b2237716",
      "6d9e05e089c84153add947b9841b636f",
      "7c3c2dbeebcf4163b7ec2d3e77e2f402",
      "19887e4566ef4fe596ad3596991a1a24",
      "849b040cf7d04bd79134accca609ed0f",
      "9d3dd5c7e01949ad998c0e2b25bdc114",
      "278ea1b60429451c8490a5e2885a59f0",
      "ad334570a90a403e9f25c43ca7d97567",
      "c8425ecbc4fa4f4b99f741f1fefd46c5",
      "29a3acca1897488382d9effe27ad0f60",
      "db8e6b086b764eafa5e35b7023690fef",
      "f9c62231830a459b833e6c22aca811b5",
      "b17fbef36cba466391bcbab1f63ef04e",
      "294adfede9e946b0aa14a396ecf99fd0",
      "6968d920e4ed497fbc9b1e9af0d1c10b",
      "c3e04a42c11c451fbec55e7d0fc8f3aa",
      "f6bdb9f25ae14e048533810c6d1f688c",
      "c95d4d3aeff84d0ab91967af94aa61e8",
      "90844e49b13a42228e80b3d6ce986350",
      "2e13f879010e4822afe3ce2551b856ba",
      "82497165f804486e957b3f012fcccf09",
      "312aae652efa4260aaf875edc9d75958",
      "770379bc823c4f4bb31f1f4c3c39cb3d",
      "d3cd0eca8fb648f6abcc8dc4135c6e9e"
     ]
    },
    "id": "RpZ8BV5fODwz",
    "outputId": "0b67bce7-46bd-4a73-814e-d854e0f3cf16"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b184470bf02a4fecbc5e9f1f6eccb7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811f153d8d86426c851a7e9263a0cbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb73adb3c7e4f758bc706df25e655ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916abdcfafc94e9d80d9f853ada87866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19998a797a2d46acbe5f4c2a7297bf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3c2dbeebcf4163b7ec2d3e77e2f402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294adfede9e946b0aa14a396ecf99fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download tokenizer and model\n",
    "\n",
    "model_name = 'google/flan-t5-base'\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uq86azvaHG5K"
   },
   "source": [
    "### Streaming Dataset\n",
    "\n",
    "Now we'll create our train and dev datasets. We're going to use Huggingface dataset objects, which allow us to load publicly available datasets directly from Huggingface, or from a local source (we'll use the CSVs stored in our Drive).\n",
    "\n",
    "When loading a dataset, we can set streaming=True, which means the data won't all get loaded at once. Instead, the trainer will only load a batch of data at a time while the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KfLr1mGDySUL"
   },
   "outputs": [],
   "source": [
    "# Create the datasets for train and validation data\n",
    "\n",
    "train_dataset = load_dataset(\"csv\", data_files=train_filepath, streaming=True)['train']\n",
    "valid_dataset = load_dataset(\"csv\", data_files=valid_filepath, streaming=True)['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z9DWDn7y1Hc"
   },
   "source": [
    "Make a preprocessing function that just takes a pair of sentences (input text and output text) from the dataset. We use the tokenizer to encode the input text into vocab_ids, that will be the inputs to the model's encoder.\n",
    "\n",
    "We also encode the output text into vocab_ids, and use those as the labels. (For training, the model will automatically create the decoder's input_ids from the labels, shifting them one token to the right and inserting the start of sequence token first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0VEFHBNBHF-C"
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_data(text_pair):\n",
    "    orig_text, target_text = text_pair\n",
    "    orig_encoded = t5_tokenizer.batch_encode_plus(\n",
    "        [orig_text],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    orig_input_ids = orig_encoded['input_ids'][0]\n",
    "    orig_attention_mask = orig_encoded['attention_mask'][0]\n",
    "\n",
    "    target_encoded = t5_tokenizer.batch_encode_plus(\n",
    "        [target_text],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    label_ids = target_encoded['input_ids'][0]\n",
    "\n",
    "    return {'input_ids': orig_input_ids,\n",
    "            'attention_mask': orig_attention_mask,\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ021Rsc0vSP"
   },
   "source": [
    "Now map the preprocessing function to the train and dev datasets. Since they're set to stream data, the preprocessing function won't be called until batches of data are dynamically loaded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FuSczzguHGDO"
   },
   "outputs": [],
   "source": [
    "# Map the preprocessing function to the datasets (it will be called when batches are loaded)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data)\n",
    "valid_dataset = valid_dataset.map(preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rag5SLP2M27"
   },
   "source": [
    "# Training\n",
    "\n",
    "Now we're ready to set up the trainer. We need a TrainingArguments class, where we specify the batch size and number of epochs. Since the streaming dataset doesn't indicate how much data there actually is, we specify the total number of batches we want to run in the argument max_steps.\n",
    "\n",
    "We also specify a filepath to save the model after training. Don't use a local filepath to the notebook's temporary memory, or this will go away when the notebook shuts down. Instead, you want to save the your fine-tuned model to a place in your mounted Drive folder. That way, you can load the model you trained later on (e.g. to continue training more epochs, and/or to do model evaluation), rather than having to start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QHGnHso9bSbI"
   },
   "outputs": [],
   "source": [
    "# Specify batch size and other training arguments\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "# Modify this filepath to where you want to save the model after fine-tuning\n",
    "dir_path = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/'\n",
    "file_path = dir_path + 't5base-finetuned-shakespeare-to-modern'\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    file_path,\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_steps=int(num_epochs * num_train_examples / batch_size),\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTxnNa163LnV"
   },
   "source": [
    "The Trainer class takes the TrainingArguments we just defined, the model to use, and the train and validation datasets (our streaming datasets).\n",
    "\n",
    "Then we just call trainer.train()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zx4fOPVJLoLn"
   },
   "outputs": [],
   "source": [
    "# Define the trainer, passing in the model, training args, and data generators\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    t5_model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "QF-x_gTibIN2",
    "outputId": "a2ad5b60-334a-471a-acac-0254a7320bb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2505' max='2505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2505/2505 48:31, Epoch 2/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.111300</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2505, training_loss=0.22204420561083135, metrics={'train_runtime': 2913.7441, 'train_samples_per_second': 13.755, 'train_steps_per_second': 0.86, 'total_flos': 6856475370061824.0, 'train_loss': 0.22204420561083135, 'epoch': 2.3325349301397207})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call train - note this will take awhile to complete the three epochs\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYwGhRWsnZj_"
   },
   "source": [
    "### Does it seem to have worked?\n",
    "\n",
    "Depending on your task, you'll add your own model evaluation after training. Here's a simple check to make sure it does seem to have fine-tuned FLAN-T5 for this new task we defined. It's not perfect, but it does seem to convert at least some of the antiquated Shakespearean style text into modern equivalents (e.g. \"Dost thou\" to \"Do you\" and  \"Makest thine\" to \"Make your\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMRlS4LqbIaf",
    "outputId": "fa9ddd08-d354-4467-d405-ed7498cfc4b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thus forth thou shalt not vex me again.']\n",
      "['Do you foresee me?']\n",
      "['Make your own dinner.']\n"
     ]
    }
   ],
   "source": [
    "# Reduce unnecessary output\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
    "                        'Dost thou foresake me?',\n",
    "                        'Makest thine own dinner.']:\n",
    "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
    "    test_output_ids = t5_model.generate(test_inputs['input_ids'].cuda())\n",
    "\n",
    "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
    "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NffN-QTEQV17"
   },
   "source": [
    "Later, we can load previously saved model checkpoints from the filepath we specified in Drive, the same way we load external pretrained models from huggingface, using the \".from_pretrained()\" method. Look in Drive for the exact checkpoint name that was saved in the directory you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4AKDOxmxzatj"
   },
   "outputs": [],
   "source": [
    "# Edit this to use the correct checkpoint name that was saved in your Drive.\n",
    "# If it saved one per epoch, you can use the latest one, or the one that\n",
    "# had the lowest validation loss during training.\n",
    "\n",
    "t5_model_saved = T5ForConditionalGeneration.from_pretrained(file_path + '/checkpoint-2505')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xR1_HTh3zavY",
    "outputId": "54e34815-e199-485f-bb82-350c72d243fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thus forth thou shalt not vex me again.']\n",
      "['Do you foresee me?']\n",
      "['Make your own dinner.']\n"
     ]
    }
   ],
   "source": [
    "# Check that it still works\n",
    "for test_input_text in ['Hence forth thou shalt not vex me e\\'er again.',\n",
    "                        'Dost thou foresake me?',\n",
    "                        'Makest thine own dinner.']:\n",
    "    test_inputs = t5_tokenizer([prefix + test_input_text], return_tensors='pt')\n",
    "    test_output_ids = t5_model_saved.generate(test_inputs['input_ids'])\n",
    "\n",
    "    print([t5_tokenizer.decode(out_ids, skip_special_tokens=True,\n",
    "                               clean_up_tokenization_spaces=False) for out_ids in test_output_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le-bVQ8I_5Ym"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
