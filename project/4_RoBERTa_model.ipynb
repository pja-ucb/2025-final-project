{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer model 1: RoBERTa\n",
        "\n",
        "**context** : RoBERTa"
      ],
      "metadata": {
        "id": "Q8--UCLzbWPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/w266_final_project/source/lyrics_df_cleaned.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "gPKJlm_scDq_",
        "outputId": "9e3d58cb-63ef-40ce-830b-22d3b03f4918"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0       track_name    track_artist  valence  \\\n",
              "0           0     Dance Monkey     Tones and I    0.513   \n",
              "1          32          ROXANNE  Arizona Zervas    0.457   \n",
              "2        1056          The Box     Roddy Ricch    0.642   \n",
              "3       33824  Blinding Lights      The Weeknd    0.345   \n",
              "4       66592         Memories        Maroon 5    0.575   \n",
              "\n",
              "                                      lyrics_snippet  track_popularity  \\\n",
              "0  They say, \"Oh my god, I see the way you shine ...               100   \n",
              "1  All for the 'Gram Bitches love the 'Gram Oh, w...                99   \n",
              "2  Pullin' out the coupe at the lot Told 'em fuck...                98   \n",
              "3  Yeah  I've been tryna call I've been on my own...                98   \n",
              "4  Here's to the ones that we got Cheers to the w...                98   \n",
              "\n",
              "           track_album_id                             track_album_name  \\\n",
              "0  0UywfDKYlyiu1b38DRrzYD  Dance Monkey (Stripped Back) / Dance Monkey   \n",
              "1  6HJDrXs0hpebaRFKA1sF90                                      ROXANNE   \n",
              "2  52u4anZbHd6UInnmHRFzba        Please Excuse Me For Being Antisocial   \n",
              "3  2ZfHkwHuoAZrlz7RMj0PDz                              Blinding Lights   \n",
              "4  3nR9B40hYLKLcR0Eph3Goc                                     Memories   \n",
              "\n",
              "  track_album_release_date  danceability  energy  key  loudness  mode  \\\n",
              "0               2019-10-17         0.824   0.588    6    -6.400     0   \n",
              "1               2019-10-10         0.621   0.601    6    -5.616     0   \n",
              "2               2019-12-06         0.896   0.586   10    -6.687     0   \n",
              "3               2019-11-29         0.513   0.796    1    -4.075     1   \n",
              "4               2019-09-20         0.764   0.320   11    -7.209     1   \n",
              "\n",
              "   speechiness  acousticness  instrumentalness  liveness    tempo  duration_ms  \n",
              "0       0.0924       0.69200          0.000104    0.1490   98.027       209438  \n",
              "1       0.1480       0.05220          0.000000    0.4600  116.735       163636  \n",
              "2       0.0559       0.10400          0.000000    0.7900  116.971       196653  \n",
              "3       0.0629       0.00147          0.000209    0.0938  171.017       201573  \n",
              "4       0.0546       0.83700          0.000000    0.0822   91.019       189486  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb9f8799-6d47-4869-85da-efcd49be53b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>track_name</th>\n",
              "      <th>track_artist</th>\n",
              "      <th>valence</th>\n",
              "      <th>lyrics_snippet</th>\n",
              "      <th>track_popularity</th>\n",
              "      <th>track_album_id</th>\n",
              "      <th>track_album_name</th>\n",
              "      <th>track_album_release_date</th>\n",
              "      <th>danceability</th>\n",
              "      <th>energy</th>\n",
              "      <th>key</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>liveness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>duration_ms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Dance Monkey</td>\n",
              "      <td>Tones and I</td>\n",
              "      <td>0.513</td>\n",
              "      <td>They say, \"Oh my god, I see the way you shine ...</td>\n",
              "      <td>100</td>\n",
              "      <td>0UywfDKYlyiu1b38DRrzYD</td>\n",
              "      <td>Dance Monkey (Stripped Back) / Dance Monkey</td>\n",
              "      <td>2019-10-17</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.588</td>\n",
              "      <td>6</td>\n",
              "      <td>-6.400</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0924</td>\n",
              "      <td>0.69200</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.1490</td>\n",
              "      <td>98.027</td>\n",
              "      <td>209438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32</td>\n",
              "      <td>ROXANNE</td>\n",
              "      <td>Arizona Zervas</td>\n",
              "      <td>0.457</td>\n",
              "      <td>All for the 'Gram Bitches love the 'Gram Oh, w...</td>\n",
              "      <td>99</td>\n",
              "      <td>6HJDrXs0hpebaRFKA1sF90</td>\n",
              "      <td>ROXANNE</td>\n",
              "      <td>2019-10-10</td>\n",
              "      <td>0.621</td>\n",
              "      <td>0.601</td>\n",
              "      <td>6</td>\n",
              "      <td>-5.616</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1480</td>\n",
              "      <td>0.05220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.4600</td>\n",
              "      <td>116.735</td>\n",
              "      <td>163636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1056</td>\n",
              "      <td>The Box</td>\n",
              "      <td>Roddy Ricch</td>\n",
              "      <td>0.642</td>\n",
              "      <td>Pullin' out the coupe at the lot Told 'em fuck...</td>\n",
              "      <td>98</td>\n",
              "      <td>52u4anZbHd6UInnmHRFzba</td>\n",
              "      <td>Please Excuse Me For Being Antisocial</td>\n",
              "      <td>2019-12-06</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.586</td>\n",
              "      <td>10</td>\n",
              "      <td>-6.687</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0559</td>\n",
              "      <td>0.10400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.7900</td>\n",
              "      <td>116.971</td>\n",
              "      <td>196653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33824</td>\n",
              "      <td>Blinding Lights</td>\n",
              "      <td>The Weeknd</td>\n",
              "      <td>0.345</td>\n",
              "      <td>Yeah  I've been tryna call I've been on my own...</td>\n",
              "      <td>98</td>\n",
              "      <td>2ZfHkwHuoAZrlz7RMj0PDz</td>\n",
              "      <td>Blinding Lights</td>\n",
              "      <td>2019-11-29</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.796</td>\n",
              "      <td>1</td>\n",
              "      <td>-4.075</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0629</td>\n",
              "      <td>0.00147</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.0938</td>\n",
              "      <td>171.017</td>\n",
              "      <td>201573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66592</td>\n",
              "      <td>Memories</td>\n",
              "      <td>Maroon 5</td>\n",
              "      <td>0.575</td>\n",
              "      <td>Here's to the ones that we got Cheers to the w...</td>\n",
              "      <td>98</td>\n",
              "      <td>3nR9B40hYLKLcR0Eph3Goc</td>\n",
              "      <td>Memories</td>\n",
              "      <td>2019-09-20</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.320</td>\n",
              "      <td>11</td>\n",
              "      <td>-7.209</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0546</td>\n",
              "      <td>0.83700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>91.019</td>\n",
              "      <td>189486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb9f8799-6d47-4869-85da-efcd49be53b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb9f8799-6d47-4869-85da-efcd49be53b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb9f8799-6d47-4869-85da-efcd49be53b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-47add1dd-a695-4a5b-9361-ee02c0d2cde2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47add1dd-a695-4a5b-9361-ee02c0d2cde2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-47add1dd-a695-4a5b-9361-ee02c0d2cde2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3717,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 168771,\n        \"min\": 0,\n        \"max\": 1549667,\n        \"num_unique_values\": 3717,\n        \"samples\": [\n          1527071,\n          1372712,\n          1257798\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3572,\n        \"samples\": [\n          \"Lily\",\n          \"I'll Stand by You\",\n          \"Look Alive (feat. Drake)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_artist\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1674,\n        \"samples\": [\n          \"Macklemore & Ryan Lewis\",\n          \"Cory Marks\",\n          \"Au/Ra\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22423904980824935,\n        \"min\": 0.0349,\n        \"max\": 0.99,\n        \"num_unique_values\": 913,\n        \"samples\": [\n          0.831,\n          0.133,\n          0.924\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lyrics_snippet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3707,\n        \"samples\": [\n          \"I know it's not very cool Standing here, serenading like a fool But I don\\u2019t care I'll gladly be a fool for you  I know the idea isn't new To fall in love with someone on first view But I don\\u2019t care I think I'll fall in love with you  I'll put on my Sunday best You pick out your favourite dress I'll take you somewhere new I'll be old fashioned for you  It's a clich\\u00e9 or so I'm told To give your jacket up to someone when it's cold But I don\\u2019t care I don\\u2019t mind the midnight air Oh, oh  I'll walk you home to your front door I\\u2019ll say farewell until the morning calls I'll be smiling a bigger smile than before  I'll put on my Sunday best You pick out your favourite dress And I'll take you somewhere new I\\u2019ll put on my Sunday best You pick out your favourite dress I'll take you somewhere new I'll be old fashioned for you  Old fashioned for you\",\n          \"Can see it from the way you looking at me You don't think I'm worth your time Don't care about the person that I might be Offended that I walk the line  So what if I'm not So what if I'm not everything you wanted me to be? So what if I am So what if I am more than you can see?  When you treat me like that, when you treat me like that It's pushin' me harder, it's pushin' me harder When you breakin' my back, when you breakin' my back I only get stronger, I only get stronger I should've walked away one year ago When you said I wouldn\\u2019t make it out alive When you treat me like that, when you treat me like that I only get stronger, I only get stronger When you treat me like that  As far as I can tell, it's kinda crazy That you even care at all Convincing everybody you can save me But you're the one who made me fall  So what if I'm not So what if I'm not everything you wanted me to be? So what if I am So what if I am more than you can see?  When you treat me like that, when you treat me like\",\n          \"Yeah, uh, East Point, smokes some dank College Park in the house Decatur Uh, Old National got the skanks Everybody, uh, know what I'm sayin' Check it  Check it Well it's the M-I-crooked letter, ain't no one better And when I'm on the microphone you best to wear your sweater 'Cause I'm cooler than a polar bear's toenails \\\"Oh hell, there he go again talking that shit\\\" Bend corners like I was a curve, I struck a nerve And now you 'bout to see this Southern playa serve I heard it's not where you're from but where you pay rent Then I heard it's not what you make but how much you spent You got me bent like elbows, amongst other things, but I'm not worried 'Cause when we step up in the party; like a mouse, you scurry So go get your fucking shine box and your sack of nickels It tickles to see you try to be like Mr. Pickles Daddy Fat Sax, B-I-G B-O-I Is that same motherfucker that took them knuckles to your eye And I try to warn you not to test but you don't listen Giving a shoutout to my Uncle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_popularity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 59,\n        \"max\": 100,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          75,\n          87,\n          92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_album_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2987,\n        \"samples\": [\n          \"19xt2EkDfNcbAdQzOVUttd\",\n          \"5lFvZh6pCTJzr9UStebyCF\",\n          \"0GPUPlsmyjRLwBKlWNPW3T\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_album_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2898,\n        \"samples\": [\n          \"Purpose (Deluxe)\",\n          \"LONEWOLF\",\n          \"Handwritten\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_album_release_date\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1431,\n        \"samples\": [\n          \"2016-11-17\",\n          \"1985-11-11\",\n          \"2013-05-17\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"danceability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1538711874863139,\n        \"min\": 0.153,\n        \"max\": 0.979,\n        \"num_unique_values\": 682,\n        \"samples\": [\n          0.38,\n          0.836,\n          0.487\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"energy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18581917673417087,\n        \"min\": 0.0167,\n        \"max\": 0.993,\n        \"num_unique_values\": 771,\n        \"samples\": [\n          0.85,\n          0.776,\n          0.221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"key\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          3,\n          8,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loudness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8964973626746033,\n        \"min\": -24.639,\n        \"max\": -0.804,\n        \"num_unique_values\": 3033,\n        \"samples\": [\n          -11.936,\n          -8.621,\n          -3.865\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mode\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speechiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1000398905202572,\n        \"min\": 0.0228,\n        \"max\": 0.918,\n        \"num_unique_values\": 1011,\n        \"samples\": [\n          0.0848,\n          0.201\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acousticness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.233310257147966,\n        \"min\": 2.32e-06,\n        \"max\": 0.983,\n        \"num_unique_values\": 1868,\n        \"samples\": [\n          0.0022,\n          0.0449\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instrumentalness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11715872350230717,\n        \"min\": 0.0,\n        \"max\": 0.957,\n        \"num_unique_values\": 1586,\n        \"samples\": [\n          8.96e-05,\n          6.51e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liveness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13487648049979462,\n        \"min\": 0.0165,\n        \"max\": 0.926,\n        \"num_unique_values\": 973,\n        \"samples\": [\n          0.421,\n          0.0222\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tempo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27.82389864185221,\n        \"min\": 37.114,\n        \"max\": 210.164,\n        \"num_unique_values\": 3478,\n        \"samples\": [\n          122.036,\n          101.943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51086,\n        \"min\": 66786,\n        \"max\": 516893,\n        \"num_unique_values\": 3458,\n        \"samples\": [\n          358053,\n          207039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmL9uSQbcD9k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465deabb"
      },
      "source": [
        "**step 1**: import necessary libraries and ID audio features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18eef737",
        "outputId": "aea162d9-a7b3-4fc6-ad29-098d8f49e36c"
      },
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "y = df['valence']\n",
        "\n",
        "audio_features = [\n",
        "    'danceability',\n",
        "    'energy',\n",
        "    'key',\n",
        "    'loudness',\n",
        "    'mode',\n",
        "    'speechiness',\n",
        "    'acousticness',\n",
        "    'instrumentalness',\n",
        "    'liveness',\n",
        "    'tempo']\n",
        "\n",
        "X_audio = df[audio_features]\n",
        "\n",
        "print(f\"Shape of X_audio: {X_audio.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_audio: (3717, 10)\n",
            "Shape of y: (3717,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6e0a61"
      },
      "source": [
        "**step 2:**: tokenize lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "768595c5",
        "outputId": "da5ec33a-1c08-4265-adfa-34184a1dacf5"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Replace NaN with an empty string or suitable placeholder before tokenization\n",
        "lyrics_series = df['lyrics_snippet'].fillna('')\n",
        "\n",
        "X_lyrics_tokenized = tokenizer(\n",
        "    lyrics_series.tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512, # RoBERTa's max sequence length\n",
        "    return_tensors='pt' # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "print(f\"Keys in X_lyrics_tokenized: {X_lyrics_tokenized.keys()}\")\n",
        "print(f\"Shape of input_ids: {X_lyrics_tokenized['input_ids'].shape}\")\n",
        "print(f\"Shape of attention_mask: {X_lyrics_tokenized['attention_mask'].shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in X_lyrics_tokenized: KeysView({'input_ids': tensor([[    0,  1213,   224,  ...,     1,     1,     1],\n",
            "        [    0,  3684,    13,  ...,     1,     1,     1],\n",
            "        [    0, 45233,   179,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0,  1106,    38,  ...,     1,     1,     1],\n",
            "        [    0,  3762,   183,  ...,     1,     1,     1],\n",
            "        [    0,  6766,    75,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
            "Shape of input_ids: torch.Size([3717, 512])\n",
            "Shape of attention_mask: torch.Size([3717, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da204282"
      },
      "source": [
        "**step 3**: train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71d3dc1",
        "outputId": "f4c678b7-7f7b-4d08-ab23-4b5c240f7089"
      },
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# convert y (valence) Series to a PyTorch tensor\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
        "\n",
        "# first split: 70% train, 30% temp (for validation and test)\n",
        "X_audio_train, X_audio_temp, X_lyrics_input_ids_train, X_lyrics_input_ids_temp, X_lyrics_attention_mask_train, X_lyrics_attention_mask_temp, y_train, y_temp = train_test_split(\n",
        "    X_audio, X_lyrics_tokenized['input_ids'], X_lyrics_tokenized['attention_mask'], y_tensor,\n",
        "    test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# second split: divide temp into 50% validation and 50% test\n",
        "X_audio_val, X_audio_test, X_lyrics_input_ids_val, X_lyrics_input_ids_test, X_lyrics_attention_mask_val, X_lyrics_attention_mask_test, y_val, y_test = train_test_split(\n",
        "    X_audio_temp, X_lyrics_input_ids_temp, X_lyrics_attention_mask_temp, y_temp,\n",
        "    test_size=0.5, random_state=42 # 0.5 of 0.3 is 0.15\n",
        ")\n",
        "\n",
        "print(\"Shape of training sets:\")\n",
        "print(f\"  X_audio_train: {X_audio_train.shape}\")\n",
        "print(f\"  X_lyrics_input_ids_train: {X_lyrics_input_ids_train.shape}\")\n",
        "print(f\"  X_lyrics_attention_mask_train: {X_lyrics_attention_mask_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "\n",
        "print(\"\\nShape of validation sets:\")\n",
        "print(f\"  X_audio_val: {X_audio_val.shape}\")\n",
        "print(f\"  X_lyrics_input_ids_val: {X_lyrics_input_ids_val.shape}\")\n",
        "print(f\"  X_lyrics_attention_mask_val: {X_lyrics_attention_mask_val.shape}\")\n",
        "print(f\"  y_val: {y_val.shape}\")\n",
        "\n",
        "print(\"\\nShape of test sets:\")\n",
        "print(f\"  X_audio_test: {X_audio_test.shape}\")\n",
        "print(f\"  X_lyrics_input_ids_test: {X_lyrics_input_ids_test.shape}\")\n",
        "print(f\"  X_lyrics_attention_mask_test: {X_lyrics_attention_mask_test.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training sets:\n",
            "  X_audio_train: (2601, 10)\n",
            "  X_lyrics_input_ids_train: torch.Size([2601, 512])\n",
            "  X_lyrics_attention_mask_train: torch.Size([2601, 512])\n",
            "  y_train: torch.Size([2601])\n",
            "\n",
            "Shape of validation sets:\n",
            "  X_audio_val: (558, 10)\n",
            "  X_lyrics_input_ids_val: torch.Size([558, 512])\n",
            "  X_lyrics_attention_mask_val: torch.Size([558, 512])\n",
            "  y_val: torch.Size([558])\n",
            "\n",
            "Shape of test sets:\n",
            "  X_audio_test: (558, 10)\n",
            "  X_lyrics_input_ids_test: torch.Size([558, 512])\n",
            "  X_lyrics_attention_mask_test: torch.Size([558, 512])\n",
            "  y_test: torch.Size([558])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaced7d"
      },
      "source": [
        "**step 4**: add classification head to enable prediction of a (0,1) variable, instead of a class label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7f61ea6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "\n",
        "class RobertaRegressionModel(nn.Module):\n",
        "    def __init__(self, roberta_model_name='roberta-base', num_labels=1):\n",
        "        super(RobertaRegressionModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name, add_pooling_layer=False)\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(self.roberta.config.hidden_size, num_labels) # Output a single valence score\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # pull the last hidden state from roberta\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "        # generate the representation for the [CLS] token (first token)\n",
        "        cls_token_representation = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # pass the cls token representation through the regression head\n",
        "        valence_prediction = self.regressor(cls_token_representation)\n",
        "\n",
        "        return valence_prediction\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52e9f99"
      },
      "source": [
        "**step 6**: create an array of input_ids (tracking word embeddings), attention (to help place emphasis on appropriate phrase) and labels (valence)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1af0509a",
        "outputId": "8b16eff2-38d8-4e7c-ea81-6f603fe0ec09"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# define a custom dataset class to allow for storage of token ids, attention values, and labels/outcome variables\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# create instances\n",
        "batch_size = 32\n",
        "\n",
        "# training Dataset and DataLoader\n",
        "train_dataset = LyricsDataset(\n",
        "    X_lyrics_input_ids_train,\n",
        "    X_lyrics_attention_mask_train,\n",
        "    y_train\n",
        ")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Validation Dataset and DataLoader\n",
        "val_dataset = LyricsDataset(\n",
        "    X_lyrics_input_ids_val,\n",
        "    X_lyrics_attention_mask_val,\n",
        "    y_val\n",
        ")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Test Dataset and DataLoader\n",
        "test_dataset = LyricsDataset(\n",
        "    X_lyrics_input_ids_test,\n",
        "    X_lyrics_attention_mask_test,\n",
        "    y_test\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"# training batches: {len(train_dataloader)}\")\n",
        "print(f\"# validation batches: {len(val_dataloader)}\")\n",
        "print(f\"# of test batches: {len(test_dataloader)}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training batches: 82\n",
            "# validation batches: 18\n",
            "# of test batches: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2a28840"
      },
      "source": [
        "**step 7**: device selection & model initialization\n",
        "\n",
        "*to ensure that our compute resources can quickly train a transformer-based model, it's critical that we're using a GPU. running this regression on a CPU would take significantly longer.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcbc008b",
        "outputId": "9ffef7ae-5534-458b-a585-5a80996ebc7d"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# initialize the model\n",
        "model = RobertaRegressionModel(num_labels=1)# for regression\n",
        "model.to(device)\n",
        "\n",
        "# set optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d589c393"
      },
      "source": [
        "**step 8**: train and evaluate lyrics-only model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23b15237",
        "outputId": "bc0d2783-de1c-40e8-8615-e54771e54985"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "print(\"initiating training loop (lyrics-only)...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(input_ids, attention_mask).squeeze(1) # Squeeze to match labels shape\n",
        "        loss = loss_fn(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculations during validation\n",
        "        for batch_idx, batch in enumerate(val_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            predictions = model(input_ids, attention_mask).squeeze(1)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initiating training loop (lyrics-only)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d90e61f7",
        "outputId": "c382dca4-6875-478e-d913-3c6ae850f59e"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "print(\"Starting evaluation on the test set...\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        predictions = model(input_ids, attention_mask).squeeze(1)\n",
        "\n",
        "        predictions_list.extend(predictions.cpu().numpy())\n",
        "        labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predictions_np = np.array(predictions_list)\n",
        "labels_np = np.array(labels_list)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2 = r2_score(labels_np, predictions_np)\n",
        "mse = mean_squared_error(labels_np, predictions_np)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(labels_np, predictions_np)\n",
        "\n",
        "print(f\"\\nTest Set Evaluation:\")\n",
        "print(f\"  R2 Score: {r2:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "print(\"Evaluation complete.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation on the test set...\n",
            "\n",
            "Test Set Evaluation:\n",
            "  R2 Score: 0.0571\n",
            "  Mean Squared Error (MSE): 0.0418\n",
            "  Root Mean Squared Error (RMSE): 0.2044\n",
            "  Mean Absolute Error (MAE): 0.1666\n",
            "Evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed9481b2"
      },
      "source": [
        "**step 9**: train and evaluate audio only model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be96885d",
        "outputId": "a4e817a8-8505-4023-f011-374fd9bef90c"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "print(\"### Audio Features Only Model Training and Evaluation ###\")\n",
        "\n",
        "# 1. Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Fit the scaler on X_audio_train and then transform all audio feature sets\n",
        "X_audio_train_scaled = scaler.fit_transform(X_audio_train)\n",
        "X_audio_val_scaled = scaler.transform(X_audio_val)\n",
        "X_audio_test_scaled = scaler.transform(X_audio_test)\n",
        "\n",
        "print(\"Audio features scaled successfully.\")\n",
        "\n",
        "# 3. Convert y_train, y_val, and y_test (PyTorch tensors) into NumPy arrays\n",
        "y_train_np = y_train.cpu().numpy()\n",
        "y_val_np = y_val.cpu().numpy()\n",
        "y_test_np = y_test.cpu().numpy()\n",
        "\n",
        "print(\"Target variables converted to NumPy arrays.\")\n",
        "\n",
        "# 4. Initialize a LinearRegression model\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# 5. Train the LinearRegression model\n",
        "linear_model.fit(X_audio_train_scaled, y_train_np)\n",
        "\n",
        "print(\"Linear Regression model trained successfully.\")\n",
        "\n",
        "# 6. Make predictions on the scaled X_audio_test\n",
        "predictions_audio_only = linear_model.predict(X_audio_test_scaled)\n",
        "\n",
        "# 7. Calculate and print evaluation metrics\n",
        "r2_audio_only = r2_score(y_test_np, predictions_audio_only)\n",
        "mse_audio_only = mean_squared_error(y_test_np, predictions_audio_only)\n",
        "rmse_audio_only = np.sqrt(mse_audio_only)\n",
        "mae_audio_only = mean_absolute_error(y_test_np, predictions_audio_only)\n",
        "\n",
        "print(f\"\\nTest Set Evaluation (Audio Features Only Model):\")\n",
        "print(f\"  R2 Score: {r2_audio_only:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_audio_only:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_audio_only:.4f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_audio_only:.4f}\")\n",
        "\n",
        "print(\"Audio Features Only Model evaluation complete.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Audio Features Only Model Training and Evaluation ###\n",
            "Audio features scaled successfully.\n",
            "Target variables converted to NumPy arrays.\n",
            "Linear Regression model trained successfully.\n",
            "\n",
            "Test Set Evaluation (Audio Features Only Model):\n",
            "  R2 Score: 0.2187\n",
            "  Mean Squared Error (MSE): 0.0346\n",
            "  Root Mean Squared Error (RMSE): 0.1860\n",
            "  Mean Absolute Error (MAE): 0.1487\n",
            "Audio Features Only Model evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 10:** as we did for the lyrics-only model, we need to create a new class to store our features of interest for the combined model. in this case, we store audio features, lyrics information, and labels"
      ],
      "metadata": {
        "id": "EsY4NY4UyK_z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b1748b",
        "outputId": "530f8eea-3681-4642-9fcb-538091581ea7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel\n",
        "\n",
        "class CombinedRegressionModel(nn.Module):\n",
        "    def __init__(self, roberta_model_name='roberta-base', num_audio_features=10, num_labels=1):\n",
        "        super(CombinedRegressionModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name, add_pooling_layer=False)\n",
        "\n",
        "        # RoBERTa-base hidden size is 768\n",
        "        roberta_output_size = self.roberta.config.hidden_size\n",
        "        combined_feature_size = roberta_output_size + num_audio_features\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(combined_feature_size, combined_feature_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1), # Dropout for regularization\n",
        "            nn.Linear(combined_feature_size, num_labels) # Output a single valence score\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features):\n",
        "        # Get the last hidden state from RoBERTa\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "        # Extract the representation for the [CLS] token (first token)\n",
        "        cls_token_representation = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Concatenate RoBERTa [CLS] token representation with audio features\n",
        "        combined_features = torch.cat((cls_token_representation, audio_features), dim=1)\n",
        "\n",
        "        # Pass the combined features through the regression head\n",
        "        valence_prediction = self.regressor(combined_features)\n",
        "\n",
        "        return valence_prediction\n",
        "\n",
        "print(\"CombinedRegressionModel class defined successfully.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CombinedRegressionModel class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d582af27",
        "outputId": "72d39f7a-357d-44ef-c14f-5fc1c7620615"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, audio_features_scaled, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        # Convert numpy array to torch tensor if it's not already\n",
        "        if isinstance(audio_features_scaled, np.ndarray):\n",
        "            self.audio_features_scaled = torch.tensor(audio_features_scaled, dtype=torch.float32)\n",
        "        else:\n",
        "            self.audio_features_scaled = audio_features_scaled\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'audio_features': self.audio_features_scaled[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# 3. Create DataLoader instances for the training, validation, and test sets\n",
        "batch_size = 16 # Keep the same batch size as the lyrics-only model\n",
        "\n",
        "# Training Dataset and DataLoader\n",
        "train_combined_dataset = CombinedDataset(\n",
        "    X_lyrics_input_ids_train,\n",
        "    X_lyrics_attention_mask_train,\n",
        "    X_audio_train_scaled, # This is already a numpy array\n",
        "    y_train\n",
        ")\n",
        "train_combined_dataloader = DataLoader(train_combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Validation Dataset and DataLoader\n",
        "val_combined_dataset = CombinedDataset(\n",
        "    X_lyrics_input_ids_val,\n",
        "    X_lyrics_attention_mask_val,\n",
        "    X_audio_val_scaled, # This is already a numpy array\n",
        "    y_val\n",
        ")\n",
        "val_combined_dataloader = DataLoader(val_combined_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Test Dataset and DataLoader\n",
        "test_combined_dataset = CombinedDataset(\n",
        "    X_lyrics_input_ids_test,\n",
        "    X_lyrics_attention_mask_test,\n",
        "    X_audio_test_scaled, # This is already a numpy array\n",
        "    y_test\n",
        ")\n",
        "test_combined_dataloader = DataLoader(test_combined_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"CombinedDataset and DataLoader instances created successfully.\")\n",
        "print(f\"Number of training batches (combined): {len(train_combined_dataloader)}\")\n",
        "print(f\"Number of validation batches (combined): {len(val_combined_dataloader)}\")\n",
        "print(f\"Number of test batches (combined): {len(test_combined_dataloader)}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CombinedDataset and DataLoader instances created successfully.\n",
            "Number of training batches (combined): 163\n",
            "Number of validation batches (combined): 35\n",
            "Number of test batches (combined): 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8a34128",
        "outputId": "6bac61f0-2a55-4030-d709-35c31767c3b8"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize the CombinedRegressionModel\n",
        "# num_audio_features should match the number of features in X_audio (10)\n",
        "model_combined = CombinedRegressionModel(num_audio_features=X_audio_train_scaled.shape[1], num_labels=1)\n",
        "model_combined.to(device)\n",
        "\n",
        "# Define optimizer and loss function for the combined model\n",
        "optimizer_combined = optim.AdamW(model_combined.parameters(), lr=2e-5)\n",
        "loss_fn_combined = nn.MSELoss()\n",
        "\n",
        "print(\"Combined Model, optimizer, and loss function initialized successfully.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Combined Model, optimizer, and loss function initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11:** train and evaluate combined model"
      ],
      "metadata": {
        "id": "NDZHy8T2ykuH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe97b321",
        "outputId": "69b6b76c-0951-48c2-f7d9-e93bd68f4f97"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training loop parameters\n",
        "num_epochs_combined = 3\n",
        "print(\"initiating combined model training loop...\")\n",
        "\n",
        "for epoch in range(num_epochs_combined):\n",
        "    model_combined.train() # Set model to training mode\n",
        "    total_train_loss_combined = 0\n",
        "    for batch_idx, batch in enumerate(train_combined_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        audio_features = batch['audio_features'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer_combined.zero_grad()\n",
        "\n",
        "        predictions = model_combined(input_ids, attention_mask, audio_features).squeeze(1) # Squeeze to match labels shape\n",
        "        loss = loss_fn_combined(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_combined.step()\n",
        "\n",
        "        total_train_loss_combined += loss.item()\n",
        "\n",
        "    avg_train_loss_combined = total_train_loss_combined / len(train_combined_dataloader)\n",
        "\n",
        "    # Validation phase for combined model\n",
        "    model_combined.eval()\n",
        "    total_val_loss_combined = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_combined_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio_features = batch['audio_features'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            predictions = model_combined(input_ids, attention_mask, audio_features).squeeze(1)\n",
        "            loss = loss_fn_combined(predictions, labels)\n",
        "\n",
        "            total_val_loss_combined += loss.item()\n",
        "\n",
        "    avg_val_loss_combined = total_val_loss_combined / len(val_combined_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_combined}, Combined Train Loss: {avg_train_loss_combined:.4f}, Combined Val Loss: {avg_val_loss_combined:.4f}\")\n",
        "\n",
        "print(\"Combined model training complete.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting combined model training loop...\n",
            "Epoch 1/3, Combined Train Loss: 0.0623, Combined Val Loss: 0.0426\n",
            "Epoch 2/3, Combined Train Loss: 0.0493, Combined Val Loss: 0.0404\n",
            "Epoch 3/3, Combined Train Loss: 0.0413, Combined Val Loss: 0.0458\n",
            "Combined model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d5ff614",
        "outputId": "a679b173-3de7-440c-af9d-6ddadcab3e59"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "print(\"Starting evaluation of the combined model on the test set...\")\n",
        "\n",
        "model_combined.eval()\n",
        "\n",
        "predictions_combined_list = []\n",
        "labels_combined_list = []\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculations during testing\n",
        "    for batch_idx, batch in enumerate(test_combined_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        audio_features = batch['audio_features'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        predictions = model_combined(input_ids, attention_mask, audio_features).squeeze(1)\n",
        "\n",
        "        predictions_combined_list.extend(predictions.cpu().numpy())\n",
        "        labels_combined_list.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predictions_combined_np = np.array(predictions_combined_list)\n",
        "labels_combined_np = np.array(labels_combined_list)\n",
        "\n",
        "# Calculate evaluation metrics for the combined model\n",
        "r2_combined = r2_score(labels_combined_np, predictions_combined_np)\n",
        "mse_combined = mean_squared_error(labels_combined_np, predictions_combined_np)\n",
        "rmse_combined = np.sqrt(mse_combined)\n",
        "mae_combined = mean_absolute_error(labels_combined_np, predictions_combined_np)\n",
        "\n",
        "print(f\"\\nTest Set Evaluation (Combined Model):\")\n",
        "print(f\"  R2 Score: {r2_combined:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_combined:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_combined:.4f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_combined:.4f}\")\n",
        "\n",
        "print(\"Combined model evaluation complete.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation of the combined model on the test set...\n",
            "\n",
            "Test Set Evaluation (Combined Model):\n",
            "  R2 Score: 0.1335\n",
            "  Mean Squared Error (MSE): 0.0384\n",
            "  Root Mean Squared Error (RMSE): 0.1959\n",
            "  Mean Absolute Error (MAE): 0.1569\n",
            "Combined model evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd8dae3b"
      },
      "source": [
        "**step 12**: model comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc68524f",
        "outputId": "7b770410-b72c-4553-c32d-5cd8c79151a3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#pull the R2, RMSE, and MAE scores from each model\n",
        "metrics = {\n",
        "    \"Lyrics Only Model\": {\n",
        "        \"R2 Score\": r2,\n",
        "        \"Mean Squared Error (MSE)\": mse,\n",
        "        \"Root Mean Squared Error (RMSE)\": rmse,\n",
        "        \"Mean Absolute Error (MAE)\": mae\n",
        "    },\n",
        "    \"Audio Features Only Model\": {\n",
        "        \"R2 Score\": r2_audio_only,\n",
        "        \"Mean Squared Error (MSE)\": mse_audio_only,\n",
        "        \"Root Mean Squared Error (RMSE)\": rmse_audio_only,\n",
        "        \"Mean Absolute Error (MAE)\": mae_audio_only\n",
        "    },\n",
        "    \"Combined Model\": {\n",
        "        \"R2 Score\": r2_combined,\n",
        "        \"Mean Squared Error (MSE)\": mse_combined,\n",
        "        \"Root Mean Squared Error (RMSE)\": rmse_combined,\n",
        "        \"Mean Absolute Error (MAE)\": mae_combined\n",
        "    },\n",
        "    \"Fine-tuned RoBERTa Model\": {\n",
        "        \"R2 Score\": r2_ft,\n",
        "        \"Mean Squared Error (MSE)\": mse_ft,\n",
        "        \"Root Mean Squared Error (RMSE)\": rmse_ft,\n",
        "        \"Mean Absolute Error (MAE)\": mae_ft\n",
        "    }\n",
        "}\n",
        "\n",
        "# create & print a df\n",
        "comparison_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
        "print(\"\\n### Model Performance Comparison ###\")\n",
        "print(comparison_df.round(4))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Model Performance Comparison ###\n",
            "                           R2 Score  Mean Squared Error (MSE)  \\\n",
            "Lyrics Only Model            0.0571                    0.0418   \n",
            "Audio Features Only Model    0.2187                    0.0346   \n",
            "Combined Model               0.1335                    0.0384   \n",
            "Fine-tuned RoBERTa Model     0.0979                    0.0400   \n",
            "\n",
            "                           Root Mean Squared Error (RMSE)  \\\n",
            "Lyrics Only Model                                  0.2044   \n",
            "Audio Features Only Model                          0.1860   \n",
            "Combined Model                                     0.1959   \n",
            "Fine-tuned RoBERTa Model                           0.1999   \n",
            "\n",
            "                           Mean Absolute Error (MAE)  \n",
            "Lyrics Only Model                             0.1666  \n",
            "Audio Features Only Model                     0.1487  \n",
            "Combined Model                                0.1569  \n",
            "Fine-tuned RoBERTa Model                      0.1634  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning RoBERTa"
      ],
      "metadata": {
        "id": "ReC7pY4H0uv8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "578ed5c8",
        "outputId": "fb0e916d-c10a-453a-8aa2-5eb665267f0a"
      },
      "source": [
        "import torch.optim as optim\n",
        "from transformers import RobertaForSequenceClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize RobertaForSequenceClassification for regression\n",
        "fine_tune_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
        "fine_tune_model.to(device)\n",
        "print(\"RobertaForSequenceClassification model initialized for fine-tuning.\")\n",
        "\n",
        "optimizer_ft = optim.AdamW(fine_tune_model.parameters(), lr=2e-5)\n",
        "loss_fn_ft = nn.MSELoss()\n",
        "\n",
        "num_epochs_ft = 3\n",
        "\n",
        "print(\"Starting fine-tuning training loop...\")\n",
        "\n",
        "for epoch in range(num_epochs_ft):\n",
        "    fine_tune_model.train()\n",
        "    total_train_loss_ft = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer_ft.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = fine_tune_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = outputs.logits.squeeze(1)\n",
        "        loss = loss_fn_ft(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_ft.step()\n",
        "\n",
        "        total_train_loss_ft += loss.item()\n",
        "\n",
        "    avg_train_loss_ft = total_train_loss_ft / len(train_dataloader)\n",
        "\n",
        "    # Validation phase for fine-tuned model\n",
        "    fine_tune_model.eval()\n",
        "    total_val_loss_ft = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = fine_tune_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = outputs.logits.squeeze(1)\n",
        "            loss = loss_fn_ft(predictions, labels)\n",
        "\n",
        "            total_val_loss_ft += loss.item()\n",
        "\n",
        "    avg_val_loss_ft = total_val_loss_ft / len(val_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_ft}, Fine-tune Train Loss: {avg_train_loss_ft:.4f}, Fine-tune Val Loss: {avg_val_loss_ft:.4f}\")\n",
        "\n",
        "print(\"Fine-tuning complete.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaForSequenceClassification model initialized for fine-tuning.\n",
            "Starting fine-tuning training loop...\n",
            "Epoch 1/3, Fine-tune Train Loss: 0.0687, Fine-tune Val Loss: 0.0536\n",
            "Epoch 2/3, Fine-tune Train Loss: 0.0566, Fine-tune Val Loss: 0.0477\n",
            "Epoch 3/3, Fine-tune Train Loss: 0.0530, Fine-tune Val Loss: 0.0447\n",
            "Fine-tuning complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a0a360b",
        "outputId": "2b2069c0-f8a8-41b8-dcc9-dfc02e8d426e"
      },
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "print(\"Starting evaluation of the fine-tuned model on the test set...\")\n",
        "\n",
        "fine_tune_model.eval()\n",
        "\n",
        "predictions_ft_list = []\n",
        "labels_ft_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader): # Reusing test_dataloader from iitial lyrics-only evaluation\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = fine_tune_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = outputs.logits.squeeze(1)\n",
        "\n",
        "        predictions_ft_list.extend(predictions.cpu().numpy())\n",
        "        labels_ft_list.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predictions_ft_np = np.array(predictions_ft_list)\n",
        "labels_ft_np = np.array(labels_ft_list)\n",
        "\n",
        "# Calculate evaluation metrics for the fine-tuned model\n",
        "r2_ft = r2_score(labels_ft_np, predictions_ft_np)\n",
        "mse_ft = mean_squared_error(labels_ft_np, predictions_ft_np)\n",
        "rmse_ft = np.sqrt(mse_ft)\n",
        "mae_ft = mean_absolute_error(labels_ft_np, predictions_ft_np)\n",
        "\n",
        "print(f\"\\nTest Set Evaluation (Fine-tuned RoBERTa Model):\")\n",
        "print(f\"  R2 Score: {r2_ft:.4f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_ft:.4f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_ft:.4f}\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_ft:.4f}\")\n",
        "\n",
        "print(\"Fine-tuned model evaluation complete.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation of the fine-tuned model on the test set...\n",
            "\n",
            "Test Set Evaluation (Fine-tuned RoBERTa Model):\n",
            "  R2 Score: 0.0979\n",
            "  Mean Squared Error (MSE): 0.0400\n",
            "  Root Mean Squared Error (RMSE): 0.1999\n",
            "  Mean Absolute Error (MAE): 0.1634\n",
            "Fine-tuned model evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4oJBgZj0tcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}